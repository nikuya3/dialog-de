diff --git a/my_tod_pretraining.py b/my_tod_pretraining.py
index 55637ef..3967fa5 100644
--- a/my_tod_pretraining.py
+++ b/my_tod_pretraining.py
@@ -30,6 +30,7 @@ from utils.utils_msre2e import *
 from utils.utils_taskmaster import *
 from utils.utils_metalwoz import *
 from utils.utils_schema import *
+from reddit.utils_reddit import prepare_data_reddit
 
 from transformers import (
     WEIGHTS_NAME,
@@ -273,8 +274,8 @@ def get_candidate_embeddings(uttr_sys_dict, tokenizer, model):
         inputs_ids = pad_sequence(inputs_ids, batch_first=True, padding_value=0)
         if torch.cuda.is_available(): inputs_ids = inputs_ids.cuda()
         with torch.no_grad():
-            outputs = model.bert(input_ids=inputs_ids, attention_mask=inputs_ids>0)
-            sequence_output = outputs[0]
+            outputs = model.forward(input_ids=inputs_ids, attention_mask=inputs_ids>0, return_dict=True, output_hidden_states=True)
+            sequence_output = outputs.hidden_states[-1]
             cls_rep = sequence_output[:, 0, :]
         for i in range(cls_rep.size(0)):
             ToD_BERT_SYS_UTTR_EMB[inputs[i].replace(" ", "")] = {
@@ -422,25 +423,25 @@ def train(args, trn_loader, dev_loader, model, tokenizer, cand_uttr_sys_dict, ot
                 labels = labels.to(args.device)
                 
                 ## Encode the context part with BERT
-                outputs = model.bert(
+                outputs = model.forward(
                     input_cont,
                     attention_mask=input_cont>0,
+                    labels=labels,
+                    return_dict=True,
+                    output_hidden_states=True
                 )
-                sequence_output = outputs[0]
-                hid_cont = sequence_output[:, 0, :] ## CLS token
-                
-                ## Calculate MLM loss for the context
-                prediction_scores = model.cls(sequence_output)
-                loss = xeloss(prediction_scores.view(-1, model.config.vocab_size), labels.view(-1))
+                loss = outputs.loss
                 loss_mlm = loss.item()
+                hid_cont = outputs.hidden_states[-1][:, 0, :] ## CLS token
                 
                 ## Encode the response part with BERT
-                outputs = model.bert(
+                outputs = model.forward(
                     input_resp,
                     attention_mask=input_resp>0,
+                    return_dict=True,
+                    output_hidden_states=True
                 )
-                sequence_output = outputs[0]
-                hid_resp = sequence_output[:, 0, :]
+                hid_resp = outputs.hidden_states[-1][:, 0, :]
                 
                 ## Calculate RCL loss
                 scores = torch.matmul(hid_cont, hid_resp.transpose(1, 0))
@@ -593,7 +594,7 @@ def evaluate(args, model, dev_loader, tokenizer, prefix=""):
 
         with torch.no_grad():
             outputs = model(inputs, 
-                            masked_lm_labels=labels,
+                            labels=labels,
                             attention_mask=inputs>0) if args.mlm else model(inputs, labels=labels)
             
             lm_loss = outputs[0]
